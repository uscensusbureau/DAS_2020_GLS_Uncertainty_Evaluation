**2020 Redistricting Data File Least Squares Estimates README File**

(last updated: April, 2025)

[1] The 2020 Redistricting Data File Least Squares Estimates experimental data product provides a way to account for uncertainty due to the United States Census Bureau's 2020 Census Disclosure Avoidance System (DAS) by allowing users to compute unbiased estimates and confidence intervals (CIs) for each tabulation included in the persons universe of the 2020 Redistricting Data File in the US, state, county, and tract geographic levels. While these estimates and CIs account for uncertainty introduced by DAS, they do not account for uncertainty in the confidential 2020 Census raw data (*e.g.,* due to undercounts and overcounts during enumeration) or errors caused by the statistical bias that is a byproduct of the post-processing steps carried out within the DAS. The estimates in this data product are unbiased in the sense that each of their expected values is equal to the corresponding confidential 2020 Census count. These estimates are found by computing the generalized least squares (GLS) estimator based on observations given by the 2020 persons universe noisy measurement files (NMFs) for both the [Redistricting Data File](https://registry.opendata.aws/census-2020-pl94-nmf/) and [Demographic and Housing Characteristics File](https://registry.opendata.aws/census-2020-dhc-nmf/) DAS production executions, as is described in more detail in [Least Squares Estimation For Hierarchical Data](https://arxiv.org/abs/2404.13164). More detail on the 2020 Redistricting Data File DAS implementation, including the noisy measurements used within this DAS execution, can be found in [The 2020 Census Disclosure Avoidance System TopDown Algorithm](https://hdsr.mitpress.mit.edu/pub/7evz361i), and more detail on the 2020 DHC DAS implementation can be found in [Disclosure Avoidance for the 2020 Census Demographic and Housing Characteristics File](https://arxiv.org/abs/2312.10863). In addition to the noisy measurements generated by these two DAS executions, the estimators also used the equality invariant constraints used within these two DAS executions to improve accuracy further (as well as inequality constraints that can be combined with one another to derive equality constraints), which is also described in [Least Squares Estimation For Hierarchical Data](https://arxiv.org/abs/2404.13164).[^1]

[^1]: The only two sets of equality constraints that were used within DAS but were not leveraged during estimation of the GLS estimator were the state total population invariants and the block level equality constraints; for more detail, see [Least Squares Estimation For Hierarchical Data](https://arxiv.org/abs/2404.13164).

[2] The GLS estimator has the lowest variance out of all unbiased estimators that are defined by linear combinations of the observations, which is a result known as Aitken's Theorem,[^2] so the estimates provided in this data product are optimal in this sense. Since the margins of error used by the CIs described below are proportional to the standard deviation of each count, this low variance results in CIs that are narrower than CIs based on any alternative estimator that is also linear and unbiased. However, these GLS count estimates do not use information from inequality constraints that cannot be reformulated as equality constraints.[^3] As a result, the published 2020 Redistricting Data File and DHC tabulations often provide more accurate counts than the count estimates described here, since they leverage information in a much larger set of inequality constraints. In contrast, one of the main advantages of the GLS estimates provided in this data product is that they have a known statistical distribution, which allows users to compute statistically valid confidence intervals for any count that is published as part of the 2020 Redistricting Data File. 

[^2]: Aitken, A. C. (1935). "On Least Squares and Linear Combinations of Observations". *Proceedings of the Royal Society of Edinburgh*. 55: 42–48. doi:[10.1017/S0370164600014346](https://doi.org/10.1017/S0370164600014346).

[^3]: In contrast, some inequality constraints used within DAS can be reformulated as equality constraints. For example, if a given tract geographic unit does not have any college/university residence halls (*i.e.*, group quarters of type 501), then the upper bound on the population in residence halls, which is defined as 99,999 times the number of residence halls in the geographic unit, is equal to zero. Since all counts produced by DAS are also constrained to be non-negative, these two inequality constraints can be combined into a constraint that ensures the population in residence halls is equal to zero. The GLS estimator does leverage inequality constraints that can be combined into equality constraints in this way.

[3] This release consists of GLS count estimates for each published query at the census tract geographic level and above, along with the standard deviations of each of these count estimates. These data are saved in hierarchically structured Parquet files, so that one can optionally load all the data for a given geographic level as a whole as a Parquet dataframe, *e.g.*, using Apache Spark. Specifically, the root-level file location contains two files, *i.e.*, `pr` and `us`, which contain the data for the geographic units in the United States and Puerto Rico, respectively. The `us` directory contains one folder for each of the geographic levels that are included in this data product for the US geographic spine (*i.e.*, `US`, `State`, `County`, and `Tract`), and, likewise, the `pr` directory contains the geographic level directories `State`, `County`, and `Tract`. (The geographic spine used for Puerto Rico includes the geographic unit corresponding to Puerto Rico as a whole in the State geographic level, so there is not a `US` geographic level in the geographic spine of Puerto Rico.) Within each of these geographic level directories is a path to each geographic unit on the tabulation spine. The final part of the path is specified in a hierarchical manner, in which the ancestor geographic units are specified by their geographic code (geocode) starting from the root of the geographic spine and then moving downward. For example, the path to the data for the first tract in the US (*i.e.*, the tract with geocode `020100` in Autauga County, Alabama, which has state and county FIPS geographic codes given by `01` and `001` respectively) is `<path of root directory>/us/Tract/State=01/County=001/Tract=020100/`. 

[4] Within each geographic unit's directory is a pipe-delimited csv file containing the count estimates and standard deviations for each published query. In addition, these locations contain a single empty `_SUCCESS` file. These are artifacts of the original process of writing the Parquet dataframes to s3 using `Spark`, and indicate that the write operation completed successfully. The columns in the csv file are named `geocode`, `query`, `estimate`, `std_dev`, and `query_description`. The `geocode` column provides the concatenated geocodes of the ancestor geographic units from the root to the relevant geographic unit, which provides a unique geographic code to identify the relevant geographic unit. For example, each row of this column for tract `020100` in Autauga County, Alabama is `01001020100`, which is formatted as `[2 digit state FIPS code][3 digit county FIPS code][6 digit census tract code]`. The remaining columns are described in more detail in Table 1.

[**Table 1: Description of Column Names in Parquet Dataframes**]

| Column Name  |   Column Type          | Description/Notes                                                                      |
|   ---         |   ---               |   ---                                                                                   |
| geocode       |   string                | String with characters taken from \{0,1,…,9\} and length of up to 11 characters         |
| query   |   string                |  The data dictionary reference name of a given query, as described in more detail [here](https://www2.census.gov/programs-surveys/decennial/2020/technical-documentation/complete-tech-docs/summary-file/2020Census_PL94_171Redistricting_NationalTechDoc.pdf).  |
| estimate |       float (64 bit)  | The scalar GLS count estimate for the query           |
| std\_dev         |   float (64 bit)  | the standard deviation of the query answer count estimate                  |
| query\_description      |   string                         | A brief description of the query              |

[5] As an example, the first several rows in these Parquet files for the queries of census tract `01001020100` are provided in Table 2.

[**Table 2: First Several Rows For Tract `01001020100`**]

|  geocode   |     query    |    estimate    |    std_dev    |    query_description   | 
|   :---       |  :---    |     :---    |     :---    |    :---   |  
|  01001020100   |     P0010001   |     1773.9018031885519   |     1.6418324422474149   |      Total  | 
|  01001020100   |     P0010002   |     1652.0434293502806   |     5.507541967888987   |     Population of one race  | 
|  01001020100   |     P0010003   |     1385.2280337755335   |     2.34418748822052   |     White alone  |  


[6] Since the GLS estimates can be accurately modeled by a Gaussian distribution with a mean given by the confidential query answer and a standard deviation given by the `std_dev` entry of the row for the query in the csv file, a 1-\alpha CI for a given query answer can be computed using the count estimate and its standard deviation using the following steps. First, a margin of error (MOE) can be computed by multiplying the standard deviation by the (1-\alpha/2) quantile of the standard normal distribution. Afterward, this MOE can be added and subtracted from the GLS query answer estimate to derive the endpoints of the CI. Afterward, the CI can be made narrower without decreasing the coverage probability using two additional steps that leverage the fact that the true query answer must be a non-negative integer. First, the endpoints can be rounded toward the center of the CI to the nearest integer. Second, each of the two endpoints of the CIs can be rounded up to zero when it is negative. As an example, the following Python function can be used to compute a CI using the data in this data product. 


```
import numpy as np
from scipy.stats import norm as gaussian


def compute_ci(estimate: float, std_dev: float, alpha: float):
    """ Returns the 1-alpha confidence interval for the non-negative and integer confidential
    query answer. The inputs 'estimate' and 'std_dev' should correspond to the values from
    the corresponding columns of these same names that are included in the 2020 Redistricting
    Data File Generalized Least Squares Confidence Intervals Experimental Data Product. The
    output of this function is a tuple with first and second elements providing the lower
    and upper endpoints of the confidence interval, respectively.
    """
    assert 0 < alpha < 1, f"alpha must be between zero and one, but instead it is {alpha}"
    moe = std_dev * gaussian.ppf(1 - alpha / 2)
    lower_endpoint = max(np.ceil(estimate - moe), 0)
    upper_endpoint = max(np.floor(estimate + moe), 0)
    return lower_endpoint, upper_endpoint
```

[7] There is a slight difference between the universe of 2020 blocks for which tabulations were published and the universe of blocks that was used for the NMFs (and thus also used for GLS estimation) that is also worth noting. This was caused by the inputs used by DAS implying that a few respondents were located in water blocks. This issue was fixed after each DAS production run was complete, during the tabulation process. Specifically, each record in each 2020 production MDF with 16 digit tabulation block geocode given by 010039900000001, 060739901000002, 060759902000001, 120759900000008, 120869900000022, 250259901010031, 360819901000002, 410079900000010, 450299901000001, 450539901000002, 481679901000039, 530099901000035, 530619900020017, 550259917020003, and 721039900130009 were mapped to the tabulation block with geocode given by 010030108002009, 060730187001280, 060750179031031, 120759704001101, 120860080002011, 250250503001006, 360810916041000, 410079511004030, 450299708003004, 450539503024009, 481677239003135, 530090019021025, 530610401004007, 550250032001000, and 721031704004006, respectively. This discrepancy does impact the estimates for the tract geographic level, since the tracts containing the latter set of blocks had the records from the former set added to them after the DAS production run was complete. 



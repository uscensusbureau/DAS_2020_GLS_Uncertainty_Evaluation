# GLS Public Release
This is a repository for a GLS estimator based on the noisy measurements generated by DAS, as described in https://arxiv.org/abs/2404.13164.

The input parameters (eg: paths, spark partition sizes, etc.) for the main algorithm are specified in a short python file called a driver, which are located in `GLS/drivers/`. The example drivers currently in the directory `drivers` all run the main estimation algorithm and then estimate the confidence intervals (CIs) for a set of geographic levels and queries. These drivers can be called using the file run_cluster.sh.

In addition to the main algorithm, this repo also provides the script `two_pass_gls_algorithm_cov_test.py`. This script generates a random spine and random query variances and then estimates the GLS estimator variance matrix using the standard approach, ie, forming the full coefficient matrix that encodes the coefficient matrices for each geographic unit in the spine. Afterward, this script iterates through all possible pairs of geographic units on the spine and verifies that the covariance between the detailed cell estimates of these two geographic units (found using the previously-computed standard GLS covariance matrix) match the derivations for this matrix provided in the paper. 

## How to Run
To run the drivers for this program:

1. Clone this repo: `git clone git@github.t26.it.census.gov:DAS/GLS-Public-Release.git`
2. Start up a cluster by logging into AWS Console and going to CloudFormation -> Stacks. On this page on the top right, click "Create Stack" -> then "With new resources (standard)".
3. On the Create Stack page, choose "Upload a template file", then "Choose file". The file you'll choose will be the `GLS_Public.template` from this repo. Then hit "Next".
4. On this page, you'll fill out the details for your cluster. The following are required ones to fill out. After you're done, hit "Next".
    - Stack Name
    - Env
    - Cluster Purpose
    - Project Purpose
    - Product Value
    - TeamValue
5. On this page, check the checkboxes at the end of the page. Hit "Next".
6. On this page, review that everything you've chosen are correct. Then start up the stack.
7. On the EMR page, wait until the cluster has finished setting up and is in the 'WAITING' stage. Copy the IP and go into the cluster `ssh -A hadoop@{CLUSTER IP}`
8. Once you're in the cluster, clone the repo again.
9. After cloning the repo, `cd` into the repo.
10. Run the environment setup: `sudo bash setup_environment.sh`. This will take care of setting up the virtual environment, the third party requirements, and the .zip file for spark.
11. After this, you're done setting up! All that's left is to run the drivers. For example, to call the PL94 driver for Puerto Rico, you can do `driver=drivers/pr_pl94.py nohup bash run_cluster.sh 2>&1 1> run_gls.log &`.
